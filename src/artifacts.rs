//! # Artifacts Module
//!
//! Responsible for generating all "sidecar" files and documentation for an exported table.
//!
//! ## Generated Files
//! - `bigquery.ddl`: `CREATE TABLE` statement for BQ.
//! - `schema.json`: JSON Schema for BQ Load jobs.
//! - `metadata.json`: Comprehensive structured metadata (stats, columns, timings).
//! - `oracle.ddl`: Original Oracle DDL (for reference).
//! - `load_command.sh`: Ready-to-run `bq load` shell command.
//! - `export.sql`: SQLcl-compatible script to reproduce the export manually.
//! - `validation.sql`: SQL script to verify data integrity in BigQuery after load.

use std::fs::File;
use std::io::Write;
use std::path::{Path, PathBuf};
use log::{info, warn};
use oracle::sql_type::OracleType;

use serde_json::json;

use crate::validation::ValidationStats;
use crate::bigquery;

/// Generates and saves all auxiliary artifacts to the `config` directory.
pub fn save_all_artifacts(
    config_dir: &Path,
    schema: &str,
    table: &str,
    columns: &[String],
    col_types: &[OracleType],
    stats: &ValidationStats,
    oracle_ddl: Option<&str>,
    duration_secs: f64,
    enable_row_hash: bool,
    field_delimiter: &str,
) -> std::io::Result<()> {
    // 1. Generate BigQuery DDL
    let bq_ddl = generate_bigquery_ddl(schema, table, columns, col_types, enable_row_hash);
    write_file(config_dir.join("bigquery.ddl"), &bq_ddl)?;

    // 1b. Generate BigQuery Schema JSON
    let schema_path = config_dir.join("schema.json");
    // Call bigquery::generate_schema with enable_row_hash
    if let Err(e) = crate::bigquery::generate_schema(columns, col_types, schema_path.to_str().unwrap(), enable_row_hash) {
        warn!("Failed to generate schema.json: {}", e);
    }
    
    // 1c. Generate Column Mapping Doc (Parity)
    let mapping_doc = generate_column_mapping_doc(schema, table, columns, col_types, enable_row_hash);
    write_file(config_dir.join("column_mapping.md"), &mapping_doc)?;

    // 2. Save Oracle DDL
    let oracle_ddl_content = oracle_ddl.unwrap_or("-- No Oracle DDL available");
    write_file(config_dir.join("oracle.ddl"), oracle_ddl_content)?;

    // 3. Generate Load Command (shell script)
    // 3. Generate Load Command (shell script)
    let load_cmd = generate_load_command(schema, table, field_delimiter);
    write_file(config_dir.join("load_command.sh"), &load_cmd)?;
    // Make executable? In Docker/Linux, we can try, but std::fs::set_permissions is unix specific.
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        let p = config_dir.join("load_command.sh");
        if let Ok(f) = File::open(&p) {
             let _ = f.set_permissions(std::fs::Permissions::from_mode(0o755));
        }
    }

    // 4. Generate Validation SQL
    let val_sql = generate_validation_sql(schema, table, stats);
    write_file(config_dir.join("validation.sql"), &val_sql)?;

    // 5. Generate Reference Export SQL
    // 5. Generate Reference Export SQL
    let export_sql = generate_export_sql(schema, table, columns, col_types, enable_row_hash, field_delimiter);
    write_file(config_dir.join("export.sql"), &export_sql)?;

    // 6. Generate Metadata JSON (Expanded)
    // We merge stats with extra info to match Python's metadata.json structure roughly
    let metadata = json!({
        "table_name": table,
        "schema_name": schema,
        "full_name": format!("{}.{}", schema, table),
        "export_timestamp": chrono::Utc::now().to_rfc3339(),
        "validation": stats,
        "export_duration_seconds": duration_secs,
        "columns": generate_column_metadata(columns, col_types, enable_row_hash),
        "oracle_ddl_file": "oracle.ddl",
        "bigquery_ddl_file": "bigquery.ddl"
    });
    let metadata_path = config_dir.join("metadata.json");
    let f = File::create(metadata_path)?;
    serde_json::to_writer_pretty(f, &metadata)?;

    info!("Saved all artifacts to {:?}", config_dir);
    Ok(())
}

fn write_file(path: PathBuf, content: &str) -> std::io::Result<()> {
    let mut f = File::create(&path)?;
    f.write_all(content.as_bytes())?;
    Ok(())
}

fn generate_bigquery_ddl(schema: &str, table: &str, columns: &[String], col_types: &[OracleType], enable_row_hash: bool) -> String {
    let mut lines = Vec::new();
    lines.push(format!("CREATE TABLE `{}.{}.{}` (", "project", schema, table)); // Placeholder project
    
    for (i, (name, otype)) in columns.iter().zip(col_types.iter()).enumerate() {
        let bq_type = bigquery::map_oracle_to_bq_ddl(otype);
        let comma = if i < columns.len() - 1 || enable_row_hash { "," } else { "" };
        lines.push(format!("  {} {}{}", name, bq_type, comma));
    }
    
    if enable_row_hash {
        lines.push("  ROW_HASH STRING NOT NULL".to_string());
    }
    
    lines.push(");".to_string());
    lines.join("\n")
}

fn generate_load_command(schema: &str, table: &str, delimiter: &str) -> String {
    format!(r#"#!/bin/bash
# BigQuery Load Command
# Generated by Oracle Rust Exporter

bq load \
    --source_format=CSV \
    --field_delimiter='{}' \
    --skip_leading_rows=1 \
    --null_marker='' \
    --allow_jagged_rows=false \
    --schema=bq_schema.json \
    {}.{} \
    "data/*.csv.gz"
"#, delimiter, schema, table)
}

fn generate_validation_sql(schema: &str, table: &str, stats: &ValidationStats) -> String {
    let row_count = stats.row_count;
    let mut sql = format!(r#"-- Validation SQL for {}.{}
-- Expected Row Count: {}

SELECT 
    'ROW_COUNT' as check_type,
    COUNT(*) as bq_value,
    {} as oracle_value,
    CASE 
        WHEN COUNT(*) = {} THEN 'PASS'
        ELSE 'FAIL'
    END as status,
    COUNT(*) - {} as diff
FROM `{}.{}.{}`;
"#, schema, table, row_count, row_count, row_count, row_count, "project", schema, table);

    // Add Aggregate Checks
    if let Some(aggs) = &stats.aggregates {
        for agg in aggs {
            if agg.agg_type == "SUM" {
                sql.push_str(&format!(r#"
SELECT 
    '{}_SUM' as check_type,
    SUM({}) as bq_value,
    {} as oracle_value,
    CASE 
        WHEN ABS(COALESCE(SUM({}), 0) - {}) < 0.01 THEN 'PASS'
        ELSE 'FAIL'
    END as status
FROM `{}.{}.{}`;
"#, agg.column_name, agg.column_name, agg.value, agg.column_name, agg.value, "project", schema, table));
            }
        }
    }
    
    // PK Hash Check (Conceptual)
    // To properly check PK Hash in BigQuery, we'd need to compute ORA_HASH in BQ which doesn't exist identically.
    // So usually DVT uses `farm_fingerprint` or `md5`. 
    // Since we can't easily replicate ORA_HASH in BQ, we just log it as a comment for manual verification if tools align.
    if let Some(pk_hash) = &stats.pk_hash {
        sql.push_str(&format!(r#"
-- CHECK PK HASH (Manual)
-- Oracle PK Hash (SUM(ORA_HASH(PK))): {}
-- BigQuery equivalent requires custom UDF or specific logic matching Oracle's ORA_HASH
"#, pk_hash));
    }

    sql
}

fn generate_export_sql(schema: &str, table: &str, columns: &[String], col_types: &[OracleType], enable_row_hash: bool, delimiter: &str) -> String {
    let mut select_exprs = Vec::new();


    for (name, otype) in columns.iter().zip(col_types.iter()) {
        // 1. Column Expression
        let expr = match otype {
            OracleType::TimestampTZ(_) | OracleType::TimestampLTZ(_) => {
                format!("TO_CHAR(SYS_EXTRACT_UTC(\"{}\"), 'YYYY-MM-DD\"T\"HH24:MI:SS.FF6\"Z\"')", name)
            },
            _ => format!("\"{}\"", name),
        };
        select_exprs.push(expr.clone());
    }

    // 2. Hash Part (Outside Loop since it aggregates all columns)
    if enable_row_hash {
        select_exprs.push(crate::sql_utils::build_row_hash_select(columns, col_types));
    }

    let cols = select_exprs.join(", ");
    
    // SQLcl Script Format
    format!(r#"-- Reference Export Script for SQLcl
SET SQLFORMAT delimited
SET DELIMITER '{}'
SET FEEDBACK OFF
SET HEAD ON
SET TRIMSPOOL ON
SET PAGESIZE 50000

SPOOL {}_{}_fallback.csv

SELECT {} FROM "{}"."{}";

SPOOL OFF
"#, delimiter, schema, table, cols, schema, table)
}

fn generate_column_metadata(columns: &[String], col_types: &[OracleType], enable_row_hash: bool) -> serde_json::Value {
    let mut cols = Vec::new();
    for (name, otype) in columns.iter().zip(col_types.iter()) {
        cols.push(json!({
            "name": name,
            "oracle_type": format!("{:?}", otype), // Debug representation
            "bigquery_type": bigquery::map_oracle_to_bq(otype)
        }));
    }
    if enable_row_hash {
        cols.push(json!({
            "name": "ROW_HASH",
            "oracle_type": "VARCHAR2 (Computed)",
            "bigquery_type": "STRING",
            "description": "SHA256 Hash of all columns"
        }));
    }
    serde_json::to_value(cols).unwrap()
}

fn generate_column_mapping_doc(schema: &str, table: &str, columns: &[String], col_types: &[OracleType], enable_row_hash: bool) -> String {
    let mut lines = Vec::new();
    lines.push(format!("# Column Mapping for {}.{}", schema, table));
    lines.push("".to_string());
    lines.push("| Oracle Column | Oracle Type | BigQuery Type |".to_string());
    lines.push("|--------------|-------------|---------------|".to_string());
    
    for (name, otype) in columns.iter().zip(col_types.iter()) {
         let bq_type = bigquery::map_oracle_to_bq(otype);
         lines.push(format!("| {} | {:?} | {} |", name, otype, bq_type));
    }
    
    if enable_row_hash {
        lines.push("| ROW_HASH | (Computed) | STRING |".to_string());
    }
    
    lines.join("\n")
}
